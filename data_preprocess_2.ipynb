{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to preprocess the data for the glider dashboard\n",
    "\n",
    "Here we process/prepare the data files for:  \n",
    "a) Sea surface fields  \n",
    "b) Glider variables  \n",
    "so they can be fed into the dashboard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import glidertools as gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sea surface variables \n",
    "\n",
    "In the case of the SOGOS glider data we will use fields of sea surface height (and derived variables) and the finite scale Lyapunov exponents (FSLE). \n",
    "\n",
    "For many some glider experiments in cloud free regions other surface fields, like surface temperature or chlorophyll concentration, might also be available. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open data\n",
    "\n",
    "# open Surface fields\n",
    "# These were obtained from the Copernicus and Aviso websites,\n",
    "# and manually cut for the time region and time period of the\n",
    "# glider deployments. \n",
    "fname_SSH = \"./data/SSH_sogos.nc\"\n",
    "fname_FSLE = \"./data/FSLE_sogos.nc\"\n",
    "\n",
    "ds_ssh = xr.open_dataset(fname_SSH)\n",
    "ds_fsle = xr.open_dataset(fname_FSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables for vectors\n",
    "# these are the variables that are needed for the quiver plot function\n",
    "ds_ssh['mag'] = np.sqrt(ds_ssh.ugos**2 + ds_ssh.vgos**2)\n",
    "ds_ssh['angle'] = (np.pi/2.) - np.arctan2(ds_ssh.ugos/ds_ssh['mag'], \n",
    "                                          ds_ssh.vgos/ds_ssh['mag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an axis with time in YTD units, as it is easier to work with.\n",
    "days = ds_ssh.time - np.datetime64('2019-01-01')\n",
    "ds_ssh['days'] = (days / np.timedelta64(1, 'D'))\n",
    "\n",
    "days = ds_fsle.time - np.datetime64('2019-01-01')\n",
    "ds_fsle['days'] = (days / np.timedelta64(1, 'D'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ds_ssh.attrs['_NCProperties']\n",
    "# need to delete because of the issue:\n",
    "# https://github.com/pydata/xarray/issues/2822"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to nc files \n",
    "ds_ssh.to_netcdf('./data/SSH_sogos_processed.nc', 'w')\n",
    "ds_fsle.to_netcdf('./data/FSLE_sogos_processed.nc', 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glider variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open glider files\n",
    "glid_folder = './data/'\n",
    "ds_CTD_659 = xr.load_dataset(glid_folder + 'sg659/CTD_659.nc')\n",
    "ds_CTD_660 = xr.load_dataset(glid_folder + 'sg660/CTD_660.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_O2_659 = xr.load_dataset(glid_folder + 'sg659/O2_659.nc')\n",
    "ds_O2_660 = xr.load_dataset(glid_folder + 'sg660/O2_660.nc')\n",
    "\n",
    "ds_Chl_659 = xr.load_dataset(glid_folder + 'sg659/Chl_659.nc')\n",
    "ds_Chl_660 = xr.load_dataset(glid_folder + 'sg660/Chl_660.nc')\n",
    "\n",
    "#ds_BBP_659 = xr.load_dataset(glid_folder + 'sg659/BBP_659.nc')\n",
    "#ds_BBP_660 = xr.load_dataset(glid_folder + 'sg660/BBP_660.nc')\n",
    "# not working with at the moment because it is missing the time axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easier to work with a days variable that is a float rather than datenum\n",
    "days = ds_CTD_659.time - np.datetime64('2019-01-01')\n",
    "ds_CTD_659['days'] = (days / np.timedelta64(1, 'D'))\n",
    "\n",
    "days = ds_O2_659.time - np.datetime64('2019-01-01')\n",
    "ds_O2_659['days'] = (days / np.timedelta64(1, 'D'))\n",
    "\n",
    "days = ds_Chl_659.time - np.datetime64('2019-01-01')\n",
    "ds_Chl_659['days'] = (days / np.timedelta64(1, 'D'))\n",
    "\n",
    "days = ds_CTD_660.time - np.datetime64('2019-01-01')\n",
    "ds_CTD_660['days'] = (days / np.timedelta64(1, 'D'))\n",
    "\n",
    "days = ds_O2_660.time - np.datetime64('2019-01-01')\n",
    "ds_O2_660['days'] = (days / np.timedelta64(1, 'D'))\n",
    "\n",
    "days = ds_Chl_660.time - np.datetime64('2019-01-01')\n",
    "ds_Chl_660['days'] = (days / np.timedelta64(1, 'D'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate along track distance\n",
    "dXdist = gt.utils.distance(ds_CTD_659.longitude, \n",
    "                           ds_CTD_659.latitude)/1e3 # Convert to km\n",
    "ds_CTD_659['distance'] = xr.DataArray(np.nancumsum(dXdist), \n",
    "                                       dims=ds_CTD_659.dims,\n",
    "                                       coords=ds_CTD_659.coords)\n",
    "\n",
    "dXdist = gt.utils.distance(ds_CTD_660.longitude, \n",
    "                           ds_CTD_660.latitude)/1e3\n",
    "ds_CTD_660['distance'] = xr.DataArray(np.nancumsum(dXdist), \n",
    "                                       dims=ds_CTD_660.dims,\n",
    "                                       coords=ds_CTD_660.coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_659_locs = xr.Dataset()\n",
    "ds_660_locs = xr.Dataset()\n",
    "\n",
    "# Group and average by dives so that plotting of positions is fast\n",
    "ds_659_diveav = ds_CTD_659.groupby('dives').mean()\n",
    "ds_660_diveav = ds_CTD_660.groupby('dives').mean()\n",
    "\n",
    "ds_659_locs['longitude'] = ds_659_diveav.longitude\n",
    "ds_659_locs['latitude']  = ds_659_diveav.latitude\n",
    "ds_659_locs['days']      = ds_659_diveav.days\n",
    "ds_659_locs['distance']  = ds_659_diveav.distance\n",
    "\n",
    "ds_660_locs['longitude'] = ds_660_diveav.longitude\n",
    "ds_660_locs['latitude']  = ds_660_diveav.latitude\n",
    "ds_660_locs['days']      = ds_660_diveav.days\n",
    "ds_660_locs['distance']  = ds_660_diveav.distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save the lat,lon, days and along track distance for each dive\n",
    "ds_659_locs.to_netcdf('./data/659_locs.nc')\n",
    "ds_660_locs.to_netcdf('./data/660_locs.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some additional vatiable\n",
    "ds_CTD_659['potdens'] = gt.physics.potential_density(ds_CTD_659.salinity, \n",
    "                                                 ds_CTD_659.temperature, \n",
    "                                                 ds_CTD_659.pressure, \n",
    "                                                 ds_CTD_659.latitude, \n",
    "                                                 ds_CTD_659.longitude)\n",
    "\n",
    "ds_CTD_660['potdens'] = gt.physics.potential_density(ds_CTD_660.salinity, \n",
    "                                                 ds_CTD_660.temperature, \n",
    "                                                 ds_CTD_660.pressure,\n",
    "                                                 ds_CTD_660.latitude, \n",
    "                                                 ds_CTD_660.longitude)\n",
    "\n",
    "# we can add mixed layer depth, N2 etc in the future versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate and grid glider data on pressure-time axis\n",
    "# There are many ways this can be done. We choose a simple linear interpolation \n",
    "# in time and pressure.\n",
    "# We could alternatively interpolate in density-time, pressure-distance, dive-pressure etc.\n",
    "# This a place where a lot of more work into GP and \n",
    "# learning the most optimal interpolation (in sense of MLE) might work. \n",
    "\n",
    "# Note this is different from what glidertools does, which does a simple binning. \n",
    "\n",
    "# In future version we could also sort based on density field before applying \n",
    "# a bit of smoothing.\n",
    "\n",
    "from scipy.interpolate import griddata\n",
    "# interpolate on pressure-time \n",
    "def interp_pres_time(ds_glid, var): \n",
    "    pres_ug = ds_glid.pressure\n",
    "    time_ug = ds_glid.days\n",
    "    \n",
    "    # convert to points values\n",
    "    points = np.stack([time_ug.values, pres_ug.values],\n",
    "                       axis=1)\n",
    "    values = ds_glid[var].values\n",
    "    \n",
    "    # remove nans\n",
    "    non_nan = np.logical_and(np.logical_and(~np.isnan(points[:,0]), \n",
    "                                      ~np.isnan(points[:,1])),\n",
    "                                      ~np.isnan(values))\n",
    "    \n",
    "    points =points[non_nan,:]\n",
    "    values =values[non_nan]\n",
    "    \n",
    "    # define grid\n",
    "    pres_grid = np.linspace(0,1000,501)\n",
    "    time_grid = np.arange(119, 207, 1/24)\n",
    "    grid_p, grid_t = np.meshgrid(pres_grid, time_grid)\n",
    "    \n",
    "    temp_grided = griddata(points, values, \n",
    "                         (grid_t, grid_p), \n",
    "                         method='linear', rescale=True)\n",
    "    \n",
    "    return xr.DataArray(temp_grided.T, \n",
    "                               dims=[\"pressure\", \"time\"],\n",
    "                          coords={\"pressure\":pres_grid,\n",
    "                                    \"time\":time_grid}).rename(var)\n",
    "\n",
    "def interp_pres_dist(ds_glid, var): \n",
    "    pres_ug = ds_glid.pressure\n",
    "    dist_ug = ds_glid.distance\n",
    "    \n",
    "    # convert to points values\n",
    "    points = np.stack([dist_ug.values, pres_ug.values],\n",
    "                       axis=1)\n",
    "    values = ds_glid[var].values\n",
    "    \n",
    "    # remove nans\n",
    "    non_nan = np.logical_and(np.logical_and(~np.isnan(points[:,0]), \n",
    "                                      ~np.isnan(points[:,1])),\n",
    "                                      ~np.isnan(values))\n",
    "    \n",
    "    points =points[non_nan,:]\n",
    "    values =values[non_nan]\n",
    "    \n",
    "    # define grid\n",
    "    pres_grid = np.linspace(0,1000,501)\n",
    "    dist_grid = np.arange(0, dist_ug.max().values, 1)\n",
    "    grid_p, grid_d = np.meshgrid(pres_grid, dist_grid)\n",
    "    \n",
    "    temp_grided = griddata(points, values, \n",
    "                         (grid_d, grid_p), \n",
    "                         method='linear', rescale=True)\n",
    "    \n",
    "    return xr.DataArray(temp_grided.T, \n",
    "                               dims=[\"pressure\", \"distance\"],\n",
    "                          coords={\"pressure\":pres_grid,\n",
    "                                    \"distance\":dist_grid}).rename(var)\n",
    "\n",
    "# apply to all useful glider variables \n",
    "# can later add in variables measured by other instruments too \n",
    "def convert_glider_time_pres(ds_glid, vars_convert= ['temperature','salinity','potdens','spice']):\n",
    "    \n",
    "    #vars_convert = ['temperature','salinity','potdens']\n",
    "    \n",
    "    ds_grid = xr.Dataset()\n",
    "    \n",
    "    for v in vars_convert:\n",
    "            ds_grid[v] = interp_pres_time(ds_glid, v)\n",
    "    \n",
    "    return ds_grid\n",
    "\n",
    "def convert_glider_dist_pres(ds_glid, vars_convert= ['temperature','salinity','potdens','spice']):\n",
    "    \n",
    "    #vars_convert = ['temperature','salinity','potdens']\n",
    "    \n",
    "    ds_grid = xr.Dataset()\n",
    "    \n",
    "    for v in vars_convert:\n",
    "            ds_grid[v] = interp_pres_dist(ds_glid, v)\n",
    "    \n",
    "    return ds_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to go from O2 point -> CTD point\n",
    "# Then can apply distance interpolation to O2 as well \n",
    "ds_O2_on_CTD_659 = xr.DataArray(np.interp(ds_CTD_659.days, ds_O2_659.days, ds_O2_659.oxygen),\n",
    "                               dims = ds_CTD_659.dims, \n",
    "                               coords = ds_CTD_659.coords).rename('oxygen')\n",
    "ds_Chl_on_CTD_659 = xr.DataArray(np.interp(ds_CTD_659.days, ds_Chl_659.days, ds_Chl_659.Chl),\n",
    "                               dims = ds_CTD_659.dims, \n",
    "                               coords = ds_CTD_659.coords).rename('Chl')\n",
    "ds_O2_on_CTD_660 = xr.DataArray(np.interp(ds_CTD_660.days, ds_O2_660.days, ds_O2_660.oxygen),\n",
    "                               dims = ds_CTD_660.dims, \n",
    "                               coords = ds_CTD_660.coords).rename('oxygen')\n",
    "ds_Chl_on_CTD_660 = xr.DataArray(np.interp(ds_CTD_660.days, ds_Chl_660.days, ds_Chl_660.Chl),\n",
    "                               dims = ds_CTD_660.dims, \n",
    "                               coords = ds_CTD_660.coords).rename('Chl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_CTD_659['oxygen'] = ds_O2_on_CTD_659\n",
    "ds_CTD_659['Chl'] = ds_Chl_on_CTD_659\n",
    "ds_CTD_660['oxygen'] = ds_O2_on_CTD_660\n",
    "ds_CTD_660['Chl'] = ds_Chl_on_CTD_660"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from point data to gridded data, can take some time\n",
    "ds_659_Tgrid = convert_glider_time_pres(ds_CTD_659, vars_convert= ['temperature','salinity','potdens', 'oxygen', 'Chl'])\n",
    "ds_660_Tgrid = convert_glider_time_pres(ds_CTD_660, vars_convert= ['temperature','salinity','potdens', 'oxygen', 'Chl'])\n",
    "\n",
    "ds_659_Dgrid = convert_glider_dist_pres(ds_CTD_659, vars_convert= ['temperature','salinity','potdens', 'oxygen', 'Chl'])\n",
    "ds_660_Dgrid = convert_glider_dist_pres(ds_CTD_660, vars_convert= ['temperature','salinity','potdens', 'oxygen', 'Chl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deprecated in favor of the above 2 step method\n",
    "# Interpolate O2 and Chl to the same points\n",
    "#ds_659_Tgrid = xr.merge([ds_659_Tgrid, interp_pres_time(ds_O2_659, 'oxygen')])\n",
    "#ds_659_Tgrid = xr.merge([ds_659_Tgrid, interp_pres_time(ds_Chl_659, 'Chl')])\n",
    "\n",
    "#ds_660_Tgrid = xr.merge([ds_660_Tgrid, interp_pres_time(ds_O2_660, 'oxygen')])\n",
    "#ds_660_Tgrid = xr.merge([ds_660_Tgrid, interp_pres_time(ds_Chl_660, 'Chl')])\n",
    "\n",
    "#ds_659_Dgrid = xr.merge([ds_659_Dgrid, interp_pres_dist(ds_O2_659, 'oxygen')])\n",
    "#ds_659_Dgrid = xr.merge([ds_659_Dgrid, interp_pres_dist(ds_Chl_659, 'Chl')])\n",
    "\n",
    "#ds_660_Dgrid = xr.merge([ds_660_Dgrid, interp_pres_dist(ds_O2_660, 'oxygen')])\n",
    "#ds_660_Dgrid = xr.merge([ds_660_Dgrid, interp_pres_dist(ds_Chl_660, 'Chl')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate an anomaly field based on time mean \n",
    "# Could be defined in more complex ways too, like choose climatology as mean.\n",
    "ds_659_Tgrid_anomaly = ds_659_Tgrid - ds_659_Tgrid.mean('time')\n",
    "ds_660_Tgrid_anomaly = ds_660_Tgrid - ds_660_Tgrid.mean('time')\n",
    "\n",
    "ds_659_Dgrid_anomaly = ds_659_Dgrid - ds_659_Dgrid.mean('distance')\n",
    "ds_660_Dgrid_anomaly = ds_660_Dgrid - ds_660_Dgrid.mean('distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the distance axis that goes with the time\n",
    "ds_659_Tgrid_loc = convert_glider_time_pres(ds_CTD_659, vars_convert= ['latitude','longitude'])\n",
    "ds_660_Tgrid_loc = convert_glider_time_pres(ds_CTD_660, vars_convert= ['latitude','longitude'])\n",
    "\n",
    "dXdist = gt.utils.distance(ds_659_Tgrid_loc.longitude.mean('pressure'), \n",
    "                           ds_659_Tgrid_loc.latitude.mean('pressure'))/1e3\n",
    "ds_659_Tgrid['distance'] = np.nancumsum(dXdist)\n",
    "ds_659_Tgrid_anomaly['distance'] = np.nancumsum(dXdist)\n",
    "\n",
    "dXdist = gt.utils.distance(ds_660_Tgrid_loc.longitude.mean('pressure'), \n",
    "                           ds_660_Tgrid_loc.latitude.mean('pressure'))/1e3\n",
    "\n",
    "ds_660_Tgrid['distance'] = np.nancumsum(dXdist)\n",
    "ds_660_Tgrid_anomaly['distance'] = np.nancumsum(dXdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the time axis that goes with the distance \n",
    "temp = convert_glider_dist_pres(ds_CTD_659, vars_convert=['days'])\n",
    "ds_659_Dgrid['time'] = temp.days.mean('pressure').values\n",
    "\n",
    "temp = convert_glider_dist_pres(ds_CTD_660, vars_convert=['days'])\n",
    "ds_660_Dgrid['time'] = temp.days.mean('pressure').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data\n",
    "data_settings = {\"zlib\": True, \"dtype\":'float32', \"complevel\": 9}\n",
    "encoding_dict = dict(temperature=data_settings,\n",
    "                     salinity=data_settings,\n",
    "                     potdens=data_settings,\n",
    "                     oxygen=data_settings,\n",
    "                     Chl=data_settings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_659_Tgrid.to_netcdf('./data/659_Tgrid.nc', encoding=encoding_dict)\n",
    "ds_660_Tgrid.to_netcdf('./data/660_Tgrid.nc', encoding=encoding_dict)\n",
    "\n",
    "ds_659_Tgrid_anomaly.to_netcdf('./data/659_Tgrid_anomaly.nc', encoding=encoding_dict)\n",
    "ds_660_Tgrid_anomaly.to_netcdf('./data/660_Tgrid_anomaly.nc', encoding=encoding_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_659_Dgrid.to_netcdf('./data/659_Dgrid.nc', encoding=encoding_dict)\n",
    "ds_660_Dgrid.to_netcdf('./data/660_Dgrid.nc', encoding=encoding_dict)\n",
    "\n",
    "ds_659_Dgrid_anomaly.to_netcdf('./data/659_Dgrid_anomaly.nc', encoding=encoding_dict)\n",
    "ds_660_Dgrid_anomaly.to_netcdf('./data/660_Dgrid_anomaly.nc', encoding=encoding_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gliderviz]",
   "language": "python",
   "name": "conda-env-gliderviz-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
